{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e14883",
   "metadata": {},
   "source": [
    "# Exercise 4 -Computer Vision\n",
    "\n",
    "\n",
    "### 4.1 - Face Detection and Tracking\n",
    "In this task you will implement face detection and tracking using OpenCV. Specifically we are utilizing Cascade classifiers which implements Viola-Jones detection algorithm.\n",
    "\n",
    "**Reference**\n",
    "- [OpenCV documentation on cascade classifier](https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "### 4.1.1\n",
    "Execute the code below to initiate the cascadee classifier and the utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed45085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb82336",
   "metadata": {},
   "source": [
    "### 4.1.2\n",
    "\n",
    "Similar to Task 3, the first step is to obtain a frame from video file and pre-processing it. \n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete prep() function below which performs following using opencv and imutils libraries. The steps already implemented are marked with a tick \"âœ“\"\n",
    "\n",
    "- [x] Takes a frame from video feed as the input\n",
    "- [ ] Resize the frame while protecting the aspect ratio (width = 600) \n",
    "- [ ] Flip the image\n",
    "- [ ] Convert the frame to grayscale image\n",
    "- [x] Return grayscale image and resized image \n",
    "\n",
    "**References**\n",
    "\n",
    "- [imutils documentation](https://github.com/PyImageSearch/imutils#imutils)\n",
    "- [Fip an array with OpenCV](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441)\n",
    "- [color conversion with OpenCV](https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce9f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(img):\n",
    "    ## ToDo 4.1.2\n",
    "    #  1. resize using  imutils.resize()\n",
    "    img = imutils.resize(img, width=600)\n",
    "    \n",
    "    #  2. flip image vertically using cv2.flip()\n",
    "    img = cv2.flip(img, 1)\n",
    "    \n",
    "    #  3. convert to gray color using cv2.cvtColor()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return gray, img    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96848eab",
   "metadata": {},
   "source": [
    "### 4.1.3\n",
    "\n",
    "In 4.1.1 we initialized an instance of cascade classifier. Tracking a face can be broken down into 3 steps as below\n",
    "\n",
    "1. Detect Faces and ROIs\n",
    "\n",
    "   The cascade classifier has a member function which can detect faces of multiple scales in a given image. The area where a face is detected becomes a region of interest (ROI) for extracting meaningful information. \n",
    "\n",
    "    **References** : \n",
    "    [Multiscale face detection member function of cascade classifier](https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html#a90fe1b7778bed4a27aa8482e1eecc116)\n",
    "\n",
    "\n",
    "2. Extract trackable features \n",
    "\n",
    "    Shi-Tomasi Corner Detector is an implementation in openCV which extracts information from the ROI input. The extracted information are points on the face which are are trackable across a sequence of moving frames (a video).\n",
    "\n",
    "    **References** : \n",
    "    [OpenCV Trackable feature extraction function(Shi-Tomasi Corner Detector)](https://docs.opencv.org/4.5.2/d4/d8c/tutorial_py_shi_tomasi.html)\n",
    "\n",
    "\n",
    "3. Calculate the optical flow\n",
    "\n",
    "    These trackable points are used to calculate the optical flow of the faces with calcOpticalFlowPyrLK() function. The tracking is visualized via OpenCV drawing tools.\n",
    "\n",
    "    **References** : \n",
    "    - [Optical Flow calculation](https://docs.opencv.org/4.5.3/d4/dee/tutorial_optical_flow.html)\n",
    "    - [OpenCV drawing functions](https://docs.opencv.org/4.5.2/dc/da5/tutorial_py_drawing_functions.html)\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete the function which perfoms following\n",
    "\n",
    "- [x] Takes grayscale image and resized image as the input\n",
    "- [x] Detect faces in graycale image using cascade classifier. detectMultiscale() function returns detected faces as rectangles ( Top left x coordinate, Top left y coordinate, width, height)\n",
    "- [ ] Draw a rectangle around detected faces using OpenCV drawing functions\n",
    "- [ ] Slice a region of interest (ROI) from grayecale image corresponding to the detections\n",
    "- [x] Extract good features to track (p0), from OpenCV goodFeaturesToTrack() function.\n",
    "- [ ] Convert the array p0 from current format [[[x1,y1],[x2,y2],....]] to --> [[x1,y1],[x2,y2],....]. Tip : print p0 to observe current format\n",
    "- [ ] The points are located with respect to the ROI coordinates. Convert them to image coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d3e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trackable_points(gray,img):\n",
    "    # 1. Use the Cascade classifier to detect faces in the grayscale image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "    \n",
    "    # 2. Drawing Rectangles and Extracting ROIs\n",
    "    \n",
    "    # 2.1 Initialize an empty list to store trackable points\n",
    "    p0=[]\n",
    "    \n",
    "    # 2.2 Check if the faces are detected\n",
    "    if len(faces) != 0:\n",
    "        ## ToDO 4.1.3\n",
    "        # for (x,y,w,h) in faces:\n",
    "        #   draw rectang\n",
    "        #   slice ROI      \n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            # 2.3 Draw a blue rectangle around the detected face\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            \n",
    "            # 2.4 Slice the grayscale image to extract ROI corresponding to the detected face\n",
    "            roi_gray = gray[y:y+h,x:x+w]\n",
    "            \n",
    "            # 2.5 Extract good features to track (p0) from each ROI\n",
    "            p0_roi = cv2.goodFeaturesToTrack(roi_gray, maxCorners=70, qualityLevel=0.001, minDistance=5)\n",
    "            p0.append(p0_roi)\n",
    "            \n",
    "        # ToDO 4.1.3\n",
    "        #  covert fromat of p0 to [[x1,y1],[x2,y2],....] \n",
    "        #  convert points from ROI to image coordinates\n",
    "        \n",
    "        # 3. Convert the array p0 from the format [[[x1, y1]], [[x2, y2]], ...] to [[x1, y1], [x2, y2], ...]\n",
    "        # Use list comprehension that flattens a list of lists\n",
    "        p0 = [item[0] for sublist in p0 for item in sublist]\n",
    "\n",
    "        # 4. Convert the points from ROI coordinates to image coordinates\n",
    "        p0 = [[x + x1, y + y1] for (x1, y1) in p0]\n",
    "        \n",
    "        # convert into float\n",
    "        p0 = np.float32(p0)\n",
    "   \n",
    "    return p0, faces, img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6c1c4",
   "metadata": {},
   "source": [
    "**Your task**\n",
    "\n",
    "Complete the do_track_face() function which perfoms following\n",
    "\n",
    "- [x] Usecv2.calcOpticalFlowPyrLK()to calculate the optical flow for tracking face\n",
    "- [ ] Select the valid points from p1. Note that  isFound == 1 for valid points \n",
    "- [ ] Return the valid points as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6754539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_track_face(gray_prev, gray, p0):\n",
    "    # calculate the optical flow for tracking face\n",
    "    p1, isFound, err = cv2.calcOpticalFlowPyrLK(gray_prev, gray, p0, \n",
    "                                                            None,\n",
    "                                                            winSize=(31,31),\n",
    "                                                            maxLevel=10,\n",
    "                                                            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03),\n",
    "                                                            flags=cv2.OPTFLOW_LK_GET_MIN_EIGENVALS,\n",
    "                                                            minEigThreshold=0.00025)\n",
    "    \n",
    "    ## ToDo 4.1.3 - Select valid points from p1\n",
    "    \n",
    "    # Convert isFound to a 1D array (i.e., removing unnecessary dimensions)\n",
    "    isFound = np.squeeze(isFound)\n",
    "    \n",
    "    # Select only the points that are found and considered valid\n",
    "    valid_points = p1[isFound==1]\n",
    "\n",
    "    # return a numpy array of selected points from p1\n",
    "    return np.array(valid_points)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740edad5",
   "metadata": {},
   "source": [
    "### 4.1.4\n",
    "\n",
    "Run the program to view the final output of face tracking. Remember to enter the correct path to video file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6d17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38626/2727540218.py:42: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.drawMarker(img, (i[0], i[1]),[255,0,0],0)\n"
     ]
    }
   ],
   "source": [
    "frame_rate = 30\n",
    "prev = 0\n",
    "gray_prev = None\n",
    "p0 = []\n",
    "\n",
    "# Either use the existing video file or camera\n",
    "\n",
    "# 1. existing video file\n",
    "cam = cv2.VideoCapture(\"Face.mp4\")\n",
    "\n",
    "# 2 camera. \n",
    "#cam = cv2.VideoCapture(0)\n",
    "\n",
    "if not cam.isOpened():\n",
    "    raise Exception(\"Could not open camera/file\")\n",
    "    \n",
    "while True:\n",
    "    time_elapsed = time.time() - prev\n",
    "    \n",
    "    if time_elapsed > 1./frame_rate:\n",
    "        \n",
    "        ret_val,img = cam.read()\n",
    "        \n",
    "        if not ret_val:\n",
    "                cam.set(cv2.CAP_PROP_POS_FRAMES, 0)  # restart video\n",
    "                gray_prev = None  # previous frame\n",
    "                p0 = []  # previous points\n",
    "                continue\n",
    "        prev = time.time()\n",
    "        \n",
    "        gray, img = prep(img)\n",
    "\n",
    "        if len(p0) <= 10: \n",
    "            p0, faces, img = get_trackable_points(gray,img)\n",
    "            gray_prev = gray.copy()\n",
    "        \n",
    "        else:\n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                p1 = do_track_face(gray_prev, gray, p0)\n",
    "            for i in p1:\n",
    "                cv2.drawMarker(img, (i[0], i[1]),[255,0,0],0)\n",
    "            p0 = p1\n",
    "                   \n",
    "        cv2.imshow('Video feed', img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "              \n",
    "cv2.destroyAllWindows()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker",
   "language": "python",
   "name": "tracker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
